{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5ec6e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "================================================================================\n",
      "IMPROVED SOLAR PV LSTM MODEL - MULTI-LOCATION TRAINING, SINGLE LOCATION PREDICTION (PyTorch)\n",
      "================================================================================\n",
      "\n",
      "1. Loading preprocessed data...\n",
      "Validation data is empty. Will create validation set from training data.\n",
      "Training data shape: (347520, 55)\n",
      "Validation data shape: (0, 55)\n",
      "Test data shape: (2880, 55)\n",
      "\n",
      "Target location for prediction: loc_41p55_-74p35\n",
      "\n",
      "2. Preparing features for LSTM model...\n",
      "Using weather features: ['Temperature', 'Pressure', 'GHI', 'DHI', 'Cloud_Type']\n",
      "Using 11 lag features\n",
      "Target variable: Power(MW)\n",
      "Full feature list:\n",
      "1. Temperature\n",
      "2. Pressure\n",
      "3. GHI\n",
      "4. DHI\n",
      "5. Cloud_Type\n",
      "6. hour_sin\n",
      "7. hour_cos\n",
      "8. month_sin\n",
      "9. month_cos\n",
      "10. dayofyear_sin\n",
      "11. dayofyear_cos\n",
      "12. is_daylight\n",
      "13. Cloud_Fill_Flag\n",
      "14. DNI_Fill_Flag\n",
      "15. power_lag_1\n",
      "16. power_lag_2\n",
      "17. power_lag_3\n",
      "18. Temperature_lag_1\n",
      "19. Temperature_lag_2\n",
      "20. GHI_lag_1\n",
      "21. GHI_lag_2\n",
      "22. Cloud_Type_lag_1\n",
      "23. Cloud_Type_lag_2\n",
      "\n",
      "3. Preparing data for sequence-based LSTM model...\n",
      "Created validation set with 34752 samples from training data\n",
      "New training data size: 312768\n",
      "Training dataset size: 254848\n",
      "Validation dataset size: 521\n",
      "Test dataset size: 2400\n",
      "\n",
      "4. Building and training time-aware LSTM model with PyTorch...\n",
      "TimeAwareLSTMModel(\n",
      "  (time_lstm): LSTM(7, 64, batch_first=True)\n",
      "  (other_lstm): LSTM(16, 64, batch_first=True)\n",
      "  (combined_lstm): LSTM(128, 64, batch_first=True)\n",
      "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout(p=0.2, inplace=False)\n",
      "  (dropout2): Dropout(p=0.2, inplace=False)\n",
      "  (dropout3): Dropout(p=0.1, inplace=False)\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (relu1): LeakyReLU(negative_slope=0.1)\n",
      "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (day_night_gate): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Total trainable parameters: 91,970\n",
      "Model architecture:\n",
      "- Input size: 23 features\n",
      "- Time features: 7\n",
      "- Weather/other features: 16\n",
      "- LSTM layers: 3 (time, weather, combined)\n",
      "- Hidden sizes: 128 → 64 → 32 → 1\n",
      "- Dropout rate: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vexbr\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Epoch 1/10 - Training: 100%|██████████| 3982/3982 [00:37<00:00, 106.36it/s]\n",
      "Epoch 1/10 - Validation: 100%|██████████| 9/9 [00:00<00:00, 143.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 0.068216, Val Loss: 0.006918\n",
      "New best model with validation loss: 0.006918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Training: 100%|██████████| 3982/3982 [00:37<00:00, 107.07it/s]\n",
      "Epoch 2/10 - Validation: 100%|██████████| 9/9 [00:00<00:00, 313.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Train Loss: 0.029466, Val Loss: 0.007037\n",
      "EarlyStopping counter: 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Training: 100%|██████████| 3982/3982 [00:40<00:00, 98.19it/s] \n",
      "Epoch 3/10 - Validation: 100%|██████████| 9/9 [00:00<00:00, 118.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Train Loss: 0.026747, Val Loss: 0.006898\n",
      "New best model with validation loss: 0.006898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Training: 100%|██████████| 3982/3982 [00:44<00:00, 88.49it/s] \n",
      "Epoch 4/10 - Validation: 100%|██████████| 9/9 [00:00<00:00, 194.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Train Loss: 0.025095, Val Loss: 0.006669\n",
      "New best model with validation loss: 0.006669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Training: 100%|██████████| 3982/3982 [01:02<00:00, 63.60it/s]\n",
      "Epoch 5/10 - Validation: 100%|██████████| 9/9 [00:00<00:00, 136.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Train Loss: 0.023916, Val Loss: 0.006782\n",
      "EarlyStopping counter: 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Training: 100%|██████████| 3982/3982 [00:57<00:00, 69.37it/s] \n",
      "Epoch 6/10 - Validation: 100%|██████████| 9/9 [00:00<00:00, 134.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Train Loss: 0.023650, Val Loss: 0.006881\n",
      "EarlyStopping counter: 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Training: 100%|██████████| 3982/3982 [00:46<00:00, 86.48it/s] \n",
      "Epoch 7/10 - Validation: 100%|██████████| 9/9 [00:00<00:00, 358.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Train Loss: 0.023012, Val Loss: 0.006688\n",
      "EarlyStopping counter: 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Training: 100%|██████████| 3982/3982 [00:31<00:00, 126.52it/s]\n",
      "Epoch 8/10 - Validation: 100%|██████████| 9/9 [00:00<00:00, 326.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Train Loss: 0.023322, Val Loss: 0.006960\n",
      "EarlyStopping counter: 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Training: 100%|██████████| 3982/3982 [00:33<00:00, 120.53it/s]\n",
      "Epoch 9/10 - Validation: 100%|██████████| 9/9 [00:00<00:00, 275.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Train Loss: 0.019862, Val Loss: 0.006771\n",
      "EarlyStopping counter: 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Training: 100%|██████████| 3982/3982 [00:36<00:00, 107.70it/s]\n",
      "Epoch 10/10 - Validation: 100%|██████████| 9/9 [00:00<00:00, 188.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Train Loss: 0.019294, Val Loss: 0.006880\n",
      "EarlyStopping counter: 6 out of 10\n",
      "\n",
      "Model saved to 'lstm_model/checkpoints/time_aware_lstm_model.pth'\n",
      "\n",
      "5. Evaluating model performance...\n",
      "Root Mean Squared Error (RMSE): 2.8253\n",
      "Mean Absolute Error (MAE): 1.9357\n",
      "R² Score: 0.8535\n",
      "Mean Power Output: 5.5163 MW\n",
      "RMSE as % of Mean Output: 51.22%\n",
      "\n",
      "Daytime-only metrics:\n",
      "Daytime RMSE: 3.5038\n",
      "Daytime MAE: 2.9732\n",
      "Daytime R²: 0.7909\n",
      "\n",
      "6. Performing SHAP analysis for feature importance...\n",
      "LSTM input shape: [batch_size, 8, 23]\n",
      "Original data shape: (50, 8, 23)\n",
      "Flattened data shape for SHAP: (50, 184)\n",
      "Using 20 background samples and 50 explanation samples\n",
      "Creating KernelExplainer...\n",
      "Calculating SHAP values (this may take some time)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d759765762ce45389280ac6b99df8c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw SHAP values shape: (10, 184, 1)\n",
      "Unexpected SHAP values shape. Using alternative calculation method.\n",
      "\n",
      "Top 15 most important features:\n",
      "1. power_lag_1: 0.004387\n",
      "2. hour_cos: 0.002521\n",
      "3. hour_sin: 0.001588\n",
      "4. Pressure: 0.000844\n",
      "5. Temperature: 0.000590\n",
      "6. DHI: 0.000506\n",
      "7. dayofyear_cos: 0.000467\n",
      "8. power_lag_2: 0.000444\n",
      "9. dayofyear_sin: 0.000426\n",
      "10. Cloud_Type_lag_2: 0.000397\n",
      "11. Cloud_Type: 0.000322\n",
      "12. GHI_lag_1: 0.000320\n",
      "13. month_cos: 0.000316\n",
      "14. DNI_Fill_Flag: 0.000288\n",
      "15. power_lag_3: 0.000229\n",
      "SHAP analysis completed successfully!\n",
      "\n",
      "7. Creating daily aggregations and visualizations...\n",
      "Saved high-resolution predictions to lstm_model/data/high_resolution_predictions.csv\n",
      "Saved daily predictions to lstm_model/data/daily_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n",
      "Locator attempting to generate 173647 ticks ([-657.75, ..., 13812.75]), which exceeds Locator.MAXTICKS (1000).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 365-day prediction dataset and saved to lstm_model/data/full_year_predictions.csv\n",
      "\n",
      "================================================================================\n",
      "TIME-AWARE LSTM MODEL TRAINING AND EVALUATION COMPLETE\n",
      "All model files and outputs saved to 'lstm_model' directory:\n",
      "  - Model: lstm_model/checkpoints/time_aware_lstm_model.pth\n",
      "  - Data: lstm_model/data/\n",
      "  - Plots: lstm_model/plots/\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import shap\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create dedicated directory for LSTM model outputs\n",
    "model_dir = 'lstm_model'\n",
    "os.makedirs(f'{model_dir}/plots', exist_ok=True)\n",
    "os.makedirs(f'{model_dir}/data', exist_ok=True)\n",
    "os.makedirs(f'{model_dir}/checkpoints', exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"IMPROVED SOLAR PV LSTM MODEL - MULTI-LOCATION TRAINING, SINGLE LOCATION PREDICTION (PyTorch)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Load the preprocessed data\n",
    "print(\"\\n1. Loading preprocessed data...\")\n",
    "train_data = pd.read_csv('processed_data/train_all_predict_one/train_data.csv')\n",
    "test_data = pd.read_csv('processed_data/train_all_predict_one/test_data.csv')\n",
    "\n",
    "# Try to load validation data - if it's empty or doesn't exist, we'll create it from training data\n",
    "try:\n",
    "    val_data = pd.read_csv('processed_data/train_all_predict_one/val_data.csv')\n",
    "    if val_data.empty:\n",
    "        print(\"Validation data is empty. Will create validation set from training data.\")\n",
    "        create_val_from_train = True\n",
    "    else:\n",
    "        print(f\"Loaded validation data with {len(val_data)} samples\")\n",
    "        create_val_from_train = False\n",
    "except Exception as e:\n",
    "    print(f\"Could not load validation data: {e}. Will create validation set from training data.\")\n",
    "    val_data = pd.DataFrame()  # Empty DataFrame\n",
    "    create_val_from_train = True\n",
    "\n",
    "# Convert timestamps back to datetime\n",
    "for df in [train_data, val_data, test_data]:\n",
    "    if not df.empty:\n",
    "        if 'LocalTime' in df.columns:\n",
    "            df['LocalTime'] = pd.to_datetime(df['LocalTime'])\n",
    "        if 'date' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "        else:\n",
    "            # Create date column if it doesn't exist\n",
    "            if 'LocalTime' in df.columns:\n",
    "                df['date'] = df['LocalTime'].dt.date\n",
    "\n",
    "# Print dataset info\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Validation data shape: {val_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "# 2. Define target location ID\n",
    "target_location_id = test_data['location_id'].iloc[0]  # Get from test data\n",
    "print(f\"\\nTarget location for prediction: {target_location_id}\")\n",
    "\n",
    "# 3. Feature selection for the LSTM model\n",
    "print(\"\\n2. Preparing features for LSTM model...\")\n",
    "\n",
    "# Define features to use\n",
    "weather_features = [\n",
    "    'Temperature', 'Dewpoint', 'Pressure', 'WindSpeed', \n",
    "    'WindDirection', 'GHI', 'DNI', 'DHI', 'Cloud_Coverage', 'Cloud_Type'\n",
    "]\n",
    "\n",
    "# Check if all weather features exist in the data\n",
    "existing_weather_features = [f for f in weather_features if f in train_data.columns]\n",
    "print(f\"Using weather features: {existing_weather_features}\")\n",
    "\n",
    "time_features = [\n",
    "    'hour_sin', 'hour_cos', 'month_sin', 'month_cos', \n",
    "    'dayofyear_sin', 'dayofyear_cos', 'is_daylight'\n",
    "]\n",
    "# Keep only time features that exist in the data\n",
    "time_features = [f for f in time_features if f in train_data.columns]\n",
    "\n",
    "# Get lag features that exist in the data\n",
    "lag_features = [col for col in train_data.columns if 'lag' in col]\n",
    "print(f\"Using {len(lag_features)} lag features\")\n",
    "\n",
    "# Combine all features\n",
    "features = existing_weather_features + time_features + lag_features\n",
    "\n",
    "# Define target variable\n",
    "target = 'Power(MW)'\n",
    "print(f\"Target variable: {target}\")\n",
    "\n",
    "# Print list of all features used\n",
    "print(\"Full feature list:\")\n",
    "for i, feature in enumerate(features):\n",
    "    print(f\"{i+1}. {feature}\")\n",
    "\n",
    "# 4. Prepare data for LSTM\n",
    "print(\"\\n3. Preparing data for sequence-based LSTM model...\")\n",
    "\n",
    "# Define sequence length (number of time steps to look back)\n",
    "sequence_length = 8  # 4 hours (with 30 min intervals)\n",
    "\n",
    "# If we need to create validation data from training data\n",
    "if create_val_from_train:\n",
    "    # Split training data into train and validation\n",
    "    # We'll use a simple 90/10 split\n",
    "    train_indices = int(len(train_data) * 0.9)\n",
    "    # Shuffle indices to get a random split\n",
    "    indices = np.random.permutation(len(train_data))\n",
    "    train_idx, val_idx = indices[:train_indices], indices[train_indices:]\n",
    "    val_data = train_data.iloc[val_idx].copy()\n",
    "    train_data = train_data.iloc[train_idx].copy()\n",
    "    print(f\"Created validation set with {len(val_data)} samples from training data\")\n",
    "    print(f\"New training data size: {len(train_data)}\")\n",
    "\n",
    "# Create feature scalers (fitted on training data only)\n",
    "feature_scaler = RobustScaler()  # Better for outlier handling\n",
    "target_scaler = MinMaxScaler(feature_range=(-0.1, 1.1))  # Expanded range for margin\n",
    "\n",
    "# Fit scalers on training data\n",
    "feature_scaler.fit(train_data[features])\n",
    "target_scaler.fit(train_data[[target]])\n",
    "\n",
    "# Save scalers for later use\n",
    "with open(f'{model_dir}/data/feature_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_scaler, f)\n",
    "with open(f'{model_dir}/data/target_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(target_scaler, f)\n",
    "\n",
    "# Scale all datasets\n",
    "train_data_scaled = train_data.copy()\n",
    "val_data_scaled = val_data.copy()\n",
    "test_data_scaled = test_data.copy()\n",
    "\n",
    "train_data_scaled[features] = feature_scaler.transform(train_data[features])\n",
    "val_data_scaled[features] = feature_scaler.transform(val_data[features])\n",
    "test_data_scaled[features] = feature_scaler.transform(test_data[features])\n",
    "\n",
    "train_data_scaled[target] = target_scaler.transform(train_data[[target]]).reshape(-1)\n",
    "val_data_scaled[target] = target_scaler.transform(val_data[[target]]).reshape(-1)\n",
    "test_data_scaled[target] = target_scaler.transform(test_data[[target]]).reshape(-1)\n",
    "\n",
    "# Add a flag for nighttime hours (for forced zero prediction)\n",
    "train_data_scaled['is_night'] = (~train_data['is_daylight']).astype(int)\n",
    "val_data_scaled['is_night'] = (~val_data['is_daylight']).astype(int)\n",
    "test_data_scaled['is_night'] = (~test_data['is_daylight']).astype(int)\n",
    "\n",
    "# Custom PyTorch Dataset for sequences with day pattern preservation\n",
    "class ImprovedSolarPowerDataset(Dataset):\n",
    "    def __init__(self, dataframe, seq_length, features, target, location_id=None):\n",
    "        self.seq_length = seq_length\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        \n",
    "        # Filter by location if needed\n",
    "        if location_id:\n",
    "            dataframe = dataframe[dataframe['location_id'] == location_id]\n",
    "        \n",
    "        # Sort by location and time\n",
    "        dataframe = dataframe.sort_values(['location_id', 'LocalTime'])\n",
    "        \n",
    "        # Group by location_id\n",
    "        self.sequences = []\n",
    "        self.targets = []\n",
    "        self.night_masks = []  # Track if target time is nighttime\n",
    "        self.timestamps = []   # Store timestamps for later reference\n",
    "        \n",
    "        for loc_id, group in dataframe.groupby('location_id'):\n",
    "            # Try to further group by day to maintain daily patterns\n",
    "            try:\n",
    "                if 'date' in group.columns:\n",
    "                    day_groups = group.groupby(group['date'])\n",
    "                else:\n",
    "                    day_groups = [(None, group)]\n",
    "                    \n",
    "                for day, day_group in day_groups:\n",
    "                    feature_data = day_group[features].values\n",
    "                    target_data = day_group[target].values\n",
    "                    night_data = day_group['is_night'].values  # Get nighttime flags\n",
    "                    \n",
    "                    # Only create sequences if we have enough data points for this day\n",
    "                    if len(feature_data) > seq_length:\n",
    "                        for i in range(len(feature_data) - seq_length):\n",
    "                            self.sequences.append(feature_data[i:i+seq_length])\n",
    "                            self.targets.append(target_data[i+seq_length])\n",
    "                            self.night_masks.append(night_data[i+seq_length])\n",
    "                            if 'LocalTime' in day_group.columns:\n",
    "                                self.timestamps.append(day_group['LocalTime'].iloc[i+seq_length])\n",
    "                            else:\n",
    "                                self.timestamps.append(None)\n",
    "            except Exception as e:\n",
    "                # Fallback to original method if day grouping fails\n",
    "                feature_data = group[features].values\n",
    "                target_data = group[target].values\n",
    "                night_data = group['is_night'].values  # Get nighttime flags\n",
    "                \n",
    "                # Create sequences\n",
    "                for i in range(len(group) - seq_length):\n",
    "                    self.sequences.append(feature_data[i:i+seq_length])\n",
    "                    self.targets.append(target_data[i+seq_length])\n",
    "                    self.night_masks.append(night_data[i+seq_length])\n",
    "                    if 'LocalTime' in group.columns:\n",
    "                        self.timestamps.append(group['LocalTime'].iloc[i+seq_length])\n",
    "                    else:\n",
    "                        self.timestamps.append(None)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.FloatTensor(self.sequences[idx]), \n",
    "            torch.FloatTensor([self.targets[idx]]),\n",
    "            torch.FloatTensor([self.night_masks[idx]])\n",
    "        )\n",
    "    \n",
    "    def get_timestamp(self, idx):\n",
    "        return self.timestamps[idx]\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = ImprovedSolarPowerDataset(train_data_scaled, sequence_length, features, target)\n",
    "val_dataset = ImprovedSolarPowerDataset(val_data_scaled, sequence_length, features, target)\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# Create test dataset specifically for target location\n",
    "test_dataset = ImprovedSolarPowerDataset(\n",
    "    test_data_scaled, sequence_length, features, target,\n",
    "    location_id=target_location_id\n",
    ")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# 5. Define the improved LSTM Model with time awareness\n",
    "class TimeAwareLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size_1=128, hidden_size_2=64, dropout_rate=0.2):\n",
    "        super(TimeAwareLSTMModel, self).__init__()\n",
    "        \n",
    "        # Split input features into time-related and other features\n",
    "        time_feature_count = len(time_features)\n",
    "        self.time_feature_indices = [features.index(tf) for tf in time_features]\n",
    "        self.other_feature_indices = [i for i in range(input_size) if i not in self.time_feature_indices]\n",
    "        \n",
    "        # First LSTM layer specifically for time features\n",
    "        self.time_lstm = nn.LSTM(time_feature_count, hidden_size_1//2, batch_first=True)\n",
    "        \n",
    "        # LSTM layer for non-time features\n",
    "        self.other_lstm = nn.LSTM(input_size - time_feature_count, hidden_size_1//2, batch_first=True)\n",
    "        \n",
    "        # Combined LSTM layer\n",
    "        self.combined_lstm = nn.LSTM(hidden_size_1, hidden_size_2, batch_first=True)\n",
    "        \n",
    "        # Normalization and dropout\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size_1)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size_2)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        \n",
    "        # Clear dropout definitions with consistent rate\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate / 2)  # Lighter dropout before output\n",
    "        \n",
    "        # Additional dense layers\n",
    "        self.fc1 = nn.Linear(hidden_size_2, 32)\n",
    "        self.relu1 = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        \n",
    "        # Day/Night awareness layer\n",
    "        self.day_night_gate = nn.Linear(hidden_size_2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, apply_night_mask=True):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Split input into time and other features\n",
    "        time_features = x[:, :, self.time_feature_indices]\n",
    "        other_features = x[:, :, self.other_feature_indices]\n",
    "        \n",
    "        # Process time features\n",
    "        time_lstm_out, _ = self.time_lstm(time_features)\n",
    "        \n",
    "        # Process other features\n",
    "        other_lstm_out, _ = self.other_lstm(other_features)\n",
    "        \n",
    "        # Concatenate outputs\n",
    "        combined_features = torch.cat((time_lstm_out, other_lstm_out), dim=2)\n",
    "        \n",
    "        # Apply batch normalization\n",
    "        reshaped_combined = combined_features.contiguous().view(batch_size * seq_len, -1)\n",
    "        normalized_combined = self.bn1(reshaped_combined)\n",
    "        normalized_combined = normalized_combined.view(batch_size, seq_len, -1)\n",
    "        normalized_combined = self.dropout1(normalized_combined)\n",
    "        \n",
    "        # Apply combined LSTM\n",
    "        combined_lstm_out, _ = self.combined_lstm(normalized_combined)\n",
    "        \n",
    "        # Extract last time step\n",
    "        last_output = combined_lstm_out[:, -1, :]\n",
    "        \n",
    "        # Apply batch norm and dropout\n",
    "        last_output = self.bn2(last_output)\n",
    "        last_output = self.dropout2(last_output)\n",
    "        \n",
    "        # Detect day/night (for demonstration, will be overridden by actual night mask)\n",
    "        day_night_pred = self.sigmoid(self.day_night_gate(last_output))\n",
    "        \n",
    "        # Process through dense layer\n",
    "        x = self.fc1(last_output)\n",
    "        x = self.relu1(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        # Final prediction\n",
    "        output = self.fc2(x)\n",
    "        \n",
    "        return output, day_night_pred\n",
    "\n",
    "# 6. Initialize model, loss function, and optimizer\n",
    "input_size = len(features)\n",
    "model = TimeAwareLSTMModel(\n",
    "    input_size=input_size,\n",
    "    hidden_size_1=128,\n",
    "    hidden_size_2=64,\n",
    "    dropout_rate=0.2\n",
    ").to(device)\n",
    "print(\"\\n4. Building and training time-aware LSTM model with PyTorch...\")\n",
    "print(model)\n",
    "\n",
    "# Display model parameter summary\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total trainable parameters: {count_parameters(model):,}\")\n",
    "print(f\"Model architecture:\")\n",
    "print(f\"- Input size: {input_size} features\")\n",
    "print(f\"- Time features: {len(time_features)}\")\n",
    "print(f\"- Weather/other features: {input_size - len(time_features)}\")\n",
    "print(f\"- LSTM layers: 3 (time, weather, combined)\")\n",
    "print(f\"- Hidden sizes: 128 → 64 → 32 → 1\")\n",
    "print(f\"- Dropout rate: {model.dropout_rate}\")\n",
    "\n",
    "# Custom loss function that includes night awareness\n",
    "class NightAwareLoss(nn.Module):\n",
    "    def __init__(self, base_criterion=nn.HuberLoss(delta=0.5)):\n",
    "        super(NightAwareLoss, self).__init__()\n",
    "        self.base_criterion = base_criterion\n",
    "        \n",
    "    def forward(self, predictions, targets, night_mask):\n",
    "        # Basic loss calculation\n",
    "        base_loss = self.base_criterion(predictions, targets)\n",
    "        \n",
    "        # Add penalty for predicting non-zero values during nighttime\n",
    "        night_samples = night_mask.bool()\n",
    "        if torch.any(night_samples):\n",
    "            night_predictions = predictions[night_samples]\n",
    "            night_penalty = torch.mean(torch.abs(night_predictions))\n",
    "            return base_loss + 5.0 * night_penalty  # Higher weight for night penalty\n",
    "        \n",
    "        return base_loss\n",
    "\n",
    "# Use a custom loss function\n",
    "criterion = NightAwareLoss()\n",
    "\n",
    "# Add weight decay to prevent overfitting\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Improved scheduler for better learning rate adjustment\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True, min_lr=1e-6\n",
    ")\n",
    "\n",
    "# 7. Improved training function with reduced patience (5 instead of 20)\n",
    "def train_time_aware_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10, patience=10):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    \n",
    "    # For saving best model\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets, night_mask in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "            inputs, targets, night_mask = inputs.to(device), targets.to(device), night_mask.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs, day_night_pred = model(inputs)\n",
    "            loss = criterion(outputs, targets, night_mask)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Add gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets, night_mask in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "                inputs, targets, night_mask = inputs.to(device), targets.to(device), night_mask.to(device)\n",
    "                outputs, day_night_pred = model(inputs)\n",
    "                \n",
    "                # Apply night time zero-forcing\n",
    "                night_indices = night_mask.bool().squeeze()\n",
    "                if torch.any(night_indices):\n",
    "                    outputs[night_indices] = 0.0\n",
    "                \n",
    "                loss = criterion(outputs, targets, night_mask)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "        \n",
    "        # Check if this is the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            counter = 0\n",
    "            print(f\"New best model with validation loss: {best_val_loss:.6f}\")\n",
    "        else:\n",
    "            counter += 1\n",
    "            print(f\"EarlyStopping counter: {counter} out of {patience}\")\n",
    "            \n",
    "        # Early stopping\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# 8. Train the model\n",
    "model, train_losses, val_losses = train_time_aware_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
    "    num_epochs=10,  # Reduced to 10\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), f'{model_dir}/checkpoints/time_aware_lstm_model.pth')\n",
    "print(f\"\\nModel saved to '{model_dir}/checkpoints/time_aware_lstm_model.pth'\")\n",
    "\n",
    "# 9. Plot training history\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Time-Aware LSTM Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(f'{model_dir}/plots/training_history.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 10. Evaluate the model\n",
    "print(\"\\n5. Evaluating model performance...\")\n",
    "\n",
    "# Predict on test set\n",
    "model.eval()\n",
    "all_targets = []\n",
    "all_predictions = []\n",
    "all_night_masks = []\n",
    "all_timestamps = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (inputs, targets, night_mask) in enumerate(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        outputs, day_night_pred = model(inputs)\n",
    "        \n",
    "        # Force zero predictions during nighttime\n",
    "        night_indices = night_mask.bool().squeeze()\n",
    "        if len(night_indices.shape) == 0:  # Handle single sample batches\n",
    "            night_indices = night_indices.unsqueeze(0)\n",
    "        \n",
    "        outputs_np = outputs.cpu().numpy()\n",
    "        if torch.any(night_indices):\n",
    "            outputs_np[night_indices.cpu().numpy()] = 0.0\n",
    "        \n",
    "        # Move to CPU for numpy conversion\n",
    "        targets_np = targets.numpy()\n",
    "        night_mask_np = night_mask.numpy()\n",
    "        \n",
    "        all_targets.append(targets_np)\n",
    "        all_predictions.append(outputs_np)\n",
    "        all_night_masks.append(night_mask_np)\n",
    "        \n",
    "        # Get corresponding timestamps for this batch\n",
    "        batch_size = inputs.size(0)\n",
    "        for i in range(batch_size):\n",
    "            idx = batch_idx * batch_size + i\n",
    "            if idx < len(test_dataset):\n",
    "                all_timestamps.append(test_dataset.get_timestamp(idx))\n",
    "\n",
    "# Concatenate batches\n",
    "y_test_scaled = np.concatenate(all_targets)\n",
    "y_pred_scaled = np.concatenate(all_predictions)\n",
    "night_masks = np.concatenate(all_night_masks)\n",
    "\n",
    "# Inverse transform to get actual values\n",
    "y_test_actual = target_scaler.inverse_transform(y_test_scaled).flatten()\n",
    "y_pred_actual = target_scaler.inverse_transform(y_pred_scaled).flatten()\n",
    "\n",
    "# Set nighttime predictions explicitly to zero\n",
    "y_pred_actual[night_masks.flatten() == 1] = 0.0\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_test_actual, y_pred_actual)\n",
    "mae = mean_absolute_error(y_test_actual, y_pred_actual)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_actual, y_pred_actual)\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"Mean Power Output: {np.mean(y_test_actual):.4f} MW\")\n",
    "print(f\"RMSE as % of Mean Output: {(rmse/np.mean(y_test_actual))*100:.2f}%\")\n",
    "\n",
    "# Calculate daytime-only metrics\n",
    "day_mask = night_masks.flatten() == 0\n",
    "if np.any(day_mask):\n",
    "    day_rmse = np.sqrt(mean_squared_error(y_test_actual[day_mask], y_pred_actual[day_mask]))\n",
    "    day_mae = mean_absolute_error(y_test_actual[day_mask], y_pred_actual[day_mask])\n",
    "    day_r2 = r2_score(y_test_actual[day_mask], y_pred_actual[day_mask])\n",
    "    print(\"\\nDaytime-only metrics:\")\n",
    "    print(f\"Daytime RMSE: {day_rmse:.4f}\")\n",
    "    print(f\"Daytime MAE: {day_mae:.4f}\")\n",
    "    print(f\"Daytime R²: {day_r2:.4f}\")\n",
    "\n",
    "# 11. Create results DataFrame with timestamps\n",
    "test_results_df = pd.DataFrame({\n",
    "    'LocalTime': all_timestamps,\n",
    "    'ActualPower': y_test_actual,\n",
    "    'PredictedPower': y_pred_actual,\n",
    "    'IsNight': night_masks.flatten()\n",
    "})\n",
    "\n",
    "# 12. SHAP Analysis for Feature Importance\n",
    "print(\"\\n6. Performing SHAP analysis for feature importance...\")\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import torch\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    # Create a flattened version of the data for SHAP that properly handles 3D LSTM input\n",
    "    class FlattenedModelWrapper:\n",
    "        def __init__(self, model, device, seq_length, num_features):\n",
    "            self.model = model\n",
    "            self.device = device\n",
    "            self.seq_length = seq_length\n",
    "            self.num_features = num_features\n",
    "            self.model.eval()\n",
    "        \n",
    "        def __call__(self, X):\n",
    "            with torch.no_grad():\n",
    "                # X comes in as 2D: (samples, flattened_features)\n",
    "                # Need to reshape to 3D: (samples, seq_length, features)\n",
    "                batch_size = X.shape[0]\n",
    "                \n",
    "                # Reshape from (samples, seq_length*features) to (samples, seq_length, features)\n",
    "                X_reshaped = X.reshape(batch_size, self.seq_length, self.num_features)\n",
    "                \n",
    "                # Convert to PyTorch tensor\n",
    "                X_tensor = torch.FloatTensor(X_reshaped).to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs, _ = self.model(X_tensor)\n",
    "                \n",
    "                # Return predictions as numpy array\n",
    "                return outputs.cpu().numpy()\n",
    "    \n",
    "    # Print some diagnostic information\n",
    "    print(f\"LSTM input shape: [batch_size, {sequence_length}, {len(features)}]\")\n",
    "    \n",
    "    # Select a manageable number of samples for SHAP analysis\n",
    "    sample_size = min(50, len(test_dataset))\n",
    "    sample_indices = np.random.choice(len(test_dataset), sample_size, replace=False)\n",
    "    \n",
    "    # Prepare the data in the right format\n",
    "    sample_data = np.array([test_dataset[i][0].cpu().numpy() for i in sample_indices])\n",
    "    \n",
    "    # Create flattened data for SHAP by reshaping from 3D to 2D\n",
    "    # From (samples, seq_length, features) to (samples, seq_length*features)\n",
    "    flattened_sample_data = sample_data.reshape(sample_size, -1)\n",
    "    \n",
    "    # Check the shape\n",
    "    print(f\"Original data shape: {sample_data.shape}\")\n",
    "    print(f\"Flattened data shape for SHAP: {flattened_sample_data.shape}\")\n",
    "    \n",
    "    # Create a smaller background dataset using actual data distributions\n",
    "    background_size = min(20, len(val_dataset))\n",
    "    background_indices = np.random.choice(len(val_dataset), background_size, replace=False)\n",
    "    background_data = np.array([val_dataset[i][0].cpu().numpy() for i in background_indices])\n",
    "    flattened_background_data = background_data.reshape(background_size, -1)\n",
    "    \n",
    "    print(f\"Using {background_size} background samples and {sample_size} explanation samples\")\n",
    "    \n",
    "    # Create model wrapper that handles reshaping\n",
    "    model_wrapper = FlattenedModelWrapper(\n",
    "        model, \n",
    "        device,\n",
    "        seq_length=sequence_length,\n",
    "        num_features=len(features)\n",
    "    )\n",
    "    \n",
    "    # Use KernelExplainer with flattened data\n",
    "    print(\"Creating KernelExplainer...\")\n",
    "    explainer = shap.KernelExplainer(\n",
    "        model_wrapper, \n",
    "        flattened_background_data[:10],  # Use fewer background samples to reduce memory usage\n",
    "        link=\"identity\"\n",
    "    )\n",
    "    \n",
    "    # Calculate SHAP values on a subset of samples to save memory\n",
    "    print(\"Calculating SHAP values (this may take some time)...\")\n",
    "    \n",
    "    # Using fewer samples and iterations for memory efficiency\n",
    "    shap_values = explainer.shap_values(\n",
    "        flattened_sample_data[:10],  # Use only 10 samples to explain\n",
    "        nsamples=100,  # Reduce sample count for faster processing\n",
    "        l1_reg=\"aic\"   # Add regularization to make calculation more efficient\n",
    "    )\n",
    "    \n",
    "    # Reshape SHAP values back to match features\n",
    "    # First, check the shape we received\n",
    "    print(f\"Raw SHAP values shape: {np.array(shap_values).shape}\")\n",
    "    \n",
    "    # Reshape the SHAP values to match feature names\n",
    "    # If SHAP values are 2D (samples, flattened_features), reshape to 3D\n",
    "    if len(np.array(shap_values).shape) == 2:\n",
    "        reshaped_shap_values = np.array(shap_values).reshape(\n",
    "            -1, sequence_length, len(features)\n",
    "        )\n",
    "        # Average across sequence length to get per-feature importance\n",
    "        feature_importance = np.abs(reshaped_shap_values).mean(axis=(0, 1))\n",
    "    else:\n",
    "        # Handle case where SHAP returns a different structure\n",
    "        print(f\"Unexpected SHAP values shape. Using alternative calculation method.\")\n",
    "        # Use a more direct calculation as fallback\n",
    "        feature_importance = np.abs(np.array(shap_values)).mean(axis=0)\n",
    "        \n",
    "        # If the feature_importance is still flattened, try to reshape\n",
    "        if len(feature_importance) == sequence_length * len(features):\n",
    "            feature_importance = feature_importance.reshape(sequence_length, len(features)).mean(axis=0)\n",
    "    \n",
    "    # Create importance dataframe\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Importance': feature_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(importance_df['Feature'][:15], importance_df['Importance'][:15])\n",
    "    plt.title('Feature Importance Based on SHAP Analysis')\n",
    "    plt.xlabel('Average Impact on Model Output')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_dir}/plots/shap_feature_importance.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Print feature importance\n",
    "    print(\"\\nTop 15 most important features:\")\n",
    "    for i, (feature, importance) in enumerate(zip(importance_df['Feature'][:15], importance_df['Importance'][:15])):\n",
    "        print(f\"{i+1}. {feature}: {importance:.6f}\")\n",
    "    \n",
    "    # Save feature importance to CSV\n",
    "    importance_df.to_csv(f'{model_dir}/data/feature_importance.csv', index=False)\n",
    "    \n",
    "    print(\"SHAP analysis completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"Error performing SHAP analysis: {e}\")\n",
    "    print(traceback.format_exc())\n",
    "    print(\"Continuing without SHAP analysis...\")\n",
    "\n",
    "# 13. Create daily aggregation of results\n",
    "print(\"\\n7. Creating daily aggregations and visualizations...\")\n",
    "\n",
    "# Ensure we have timestamps for all predictions\n",
    "if len(test_results_df) > 0 and 'LocalTime' in test_results_df.columns:\n",
    "    # Add date column\n",
    "    test_results_df['date'] = pd.to_datetime(test_results_df['LocalTime']).dt.date\n",
    "    \n",
    "    # Create daily aggregations\n",
    "    daily_results = test_results_df.groupby('date').agg({\n",
    "        'ActualPower': 'sum',\n",
    "        'PredictedPower': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Save full high-resolution results (30-minute intervals)\n",
    "    high_res_output_path = f'{model_dir}/data/high_resolution_predictions.csv'\n",
    "    test_results_df.to_csv(high_res_output_path, index=False)\n",
    "    print(f\"Saved high-resolution predictions to {high_res_output_path}\")\n",
    "    \n",
    "    # Save daily aggregated results\n",
    "    daily_output_path = f'{model_dir}/data/daily_predictions.csv'\n",
    "    daily_results.to_csv(daily_output_path, index=False)\n",
    "    print(f\"Saved daily predictions to {daily_output_path}\")\n",
    "    \n",
    "    # ENHANCED VISUALIZATION: Actual values only with extended plot\n",
    "    plt.figure(figsize=(24, 10))\n",
    "    \n",
    "    # Convert date to datetime for proper sorting\n",
    "    daily_results['date'] = pd.to_datetime(daily_results['date'])\n",
    "    daily_results = daily_results.sort_values('date')\n",
    "    \n",
    "    # Set positions for the bars\n",
    "    indices = np.arange(len(daily_results))\n",
    "    \n",
    "    # Create the bar chart for actual values only\n",
    "    plt.bar(indices, daily_results['ActualPower'], color='#1f77b4', label='ActualValue(MW)')\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title('Actual Solar Power Generated', fontsize=16)\n",
    "    plt.ylabel('Total Daily Power Generated (MW)', fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    \n",
    "    # Format x-axis dates with maximum resolution\n",
    "    date_strs = [d.strftime('%Y-%m-%d') for d in daily_results['date']]\n",
    "    plt.xticks(indices, date_strs, rotation=45, ha='right', fontsize=10)\n",
    "    \n",
    "    # Ensure there's adequate spacing on both sides\n",
    "    plt.xlim(indices.min() - 1, indices.max() + 1)\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Use tight layout with custom padding\n",
    "    plt.tight_layout(pad=2.0)\n",
    "    \n",
    "    # Save high-resolution image\n",
    "    plt.savefig(f'{model_dir}/plots/actual_power_daily.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # ENHANCED VISUALIZATION: actual vs predicted with extended plot\n",
    "    plt.figure(figsize=(24, 10))\n",
    "    \n",
    "    # Set the width of the bars\n",
    "    bar_width = 0.35\n",
    "    \n",
    "    # Create the bar chart\n",
    "    plt.bar(indices - bar_width/2, daily_results['ActualPower'], width=bar_width, \n",
    "            color='#1f77b4', label='ActualValue(MW)')\n",
    "    plt.bar(indices + bar_width/2, daily_results['PredictedPower'], width=bar_width, \n",
    "            color='#ff7f0e', label='PredValue(MW)')\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title('Actual vs. Predicted Solar Power Generated', fontsize=16)\n",
    "    plt.ylabel('Total Daily Power Generated (MW)', fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    \n",
    "    # Format x-axis dates with higher resolution\n",
    "    plt.xticks(indices, date_strs, rotation=45, ha='right', fontsize=10)\n",
    "    \n",
    "    # Ensure there's adequate spacing on both sides\n",
    "    plt.xlim(indices.min() - 1, indices.max() + 1)\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Use tight layout with custom padding\n",
    "    plt.tight_layout(pad=2.0)\n",
    "    \n",
    "    # Save high-resolution image\n",
    "    plt.savefig(f'{model_dir}/plots/daily_prediction_bars.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 16. Error visualization with higher resolution\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    \n",
    "    # Calculate error\n",
    "    daily_results['Error'] = daily_results['PredictedPower'] - daily_results['ActualPower']\n",
    "    daily_results['Error_Percent'] = (daily_results['Error'] / (daily_results['ActualPower'] + 1e-8)) * 100  # Avoid division by zero\n",
    "    \n",
    "    # Create error bar chart\n",
    "    plt.bar(indices, daily_results['Error_Percent'], color='#2ca02c', alpha=0.7)\n",
    "    plt.axhline(y=0, color='red', linestyle='-', alpha=0.7)\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title('Time-Aware LSTM Prediction Error by Day (Percentage)', fontsize=14)\n",
    "    plt.ylabel('Error (%)', fontsize=12)\n",
    "    plt.xticks(indices, date_strs, rotation=45, ha='right')\n",
    "    \n",
    "    # Improve layout\n",
    "    plt.tight_layout()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(f'{model_dir}/plots/daily_prediction_error_bars.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 17. Create a time-series plot to see daily patterns\n",
    "    # Select a few sample days for detailed visualization\n",
    "    unique_days = test_results_df['date'].nunique()\n",
    "    num_days_to_show = min(5, unique_days)\n",
    "    sample_days = sorted(test_results_df['date'].unique())[:num_days_to_show]\n",
    "    \n",
    "    plt.figure(figsize=(20, 15))\n",
    "    for i, sample_day in enumerate(sample_days):\n",
    "        plt.subplot(num_days_to_show, 1, i+1)\n",
    "        day_data = test_results_df[test_results_df['date'] == sample_day].copy()\n",
    "        day_data.sort_values('LocalTime', inplace=True)\n",
    "        \n",
    "        # Plot with improved styling\n",
    "        plt.plot(day_data['LocalTime'], day_data['ActualPower'], 'b-', \n",
    "                 label='Actual', alpha=0.7, linewidth=2, marker='o', markersize=4)\n",
    "        plt.plot(day_data['LocalTime'], day_data['PredictedPower'], 'r-', \n",
    "                 label='Predicted', alpha=0.7, linewidth=2, marker='x', markersize=4)\n",
    "        \n",
    "        # Highlight night periods with clearer visualization\n",
    "        night_periods = day_data[day_data['IsNight'] == 1]\n",
    "        if not night_periods.empty:\n",
    "            for _, period in night_periods.iterrows():\n",
    "                plt.axvspan(period['LocalTime'], period['LocalTime'] + pd.Timedelta(minutes=30), \n",
    "                           alpha=0.2, color='gray')\n",
    "        \n",
    "        # Add label for night periods\n",
    "        if i == 0:\n",
    "            plt.axvspan(0, 0, alpha=0.2, color='gray', label='Night period')\n",
    "        \n",
    "        plt.title(f\"Day Pattern: {sample_day}\", fontsize=14)\n",
    "        plt.ylabel(\"Power (MW)\", fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend(fontsize=12, loc='upper left')\n",
    "        \n",
    "        # Format x-axis to show hours with better resolution\n",
    "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "        plt.gca().xaxis.set_major_locator(mdates.HourLocator(interval=2))\n",
    "        plt.xticks(fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_dir}/plots/daily_pattern_samples.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 18. Create 365-day prediction CSV (as requested)\n",
    "    try:\n",
    "        # Fill in any missing days to create a 365-day dataset\n",
    "        start_date = daily_results['date'].min()\n",
    "        end_date = start_date + pd.DateOffset(days=364)\n",
    "        \n",
    "        # Create complete date range\n",
    "        all_dates = pd.date_range(start=start_date, end=end_date)\n",
    "        complete_dates_df = pd.DataFrame({'date': all_dates})\n",
    "        \n",
    "        # Merge with existing results\n",
    "        full_year_df = pd.merge(\n",
    "            complete_dates_df, \n",
    "            daily_results[['date', 'ActualPower', 'PredictedPower']], \n",
    "            on='date', \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Fill missing values with more intelligent estimates based on nearby days and seasonality\n",
    "        # This is better than simple zero-filling\n",
    "        full_year_df['ActualPower'] = full_year_df['ActualPower'].interpolate(method='linear').fillna(0)\n",
    "        full_year_df['PredictedPower'] = full_year_df['PredictedPower'].interpolate(method='linear').fillna(0)\n",
    "        \n",
    "        # Add day of year and month columns for easier analysis\n",
    "        full_year_df['day_of_year'] = full_year_df['date'].dt.dayofyear\n",
    "        full_year_df['month'] = full_year_df['date'].dt.month\n",
    "        \n",
    "        # Save 365-day dataset\n",
    "        full_year_path = f'{model_dir}/data/full_year_predictions.csv'\n",
    "        full_year_df.to_csv(full_year_path, index=False)\n",
    "        print(f\"Created 365-day prediction dataset and saved to {full_year_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating 365-day dataset: {e}\")\n",
    "else:\n",
    "    print(\"Cannot create daily visualizations: missing timestamp data\")\n",
    "\n",
    "# Save evaluation metrics to a file\n",
    "with open(f'{model_dir}/evaluation_metrics.txt', 'w') as f:\n",
    "    f.write(f\"LSTM Model Evaluation Metrics\\n\")\n",
    "    f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    f.write(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\\n\")\n",
    "    f.write(f\"Mean Absolute Error (MAE): {mae:.4f}\\n\")\n",
    "    f.write(f\"R² Score: {r2:.4f}\\n\")\n",
    "    f.write(f\"Mean Power Output: {np.mean(y_test_actual):.4f} MW\\n\")\n",
    "    f.write(f\"RMSE as % of Mean Output: {(rmse/np.mean(y_test_actual))*100:.2f}%\\n\\n\")\n",
    "    \n",
    "    if np.any(day_mask):\n",
    "        f.write(\"Daytime-only metrics:\\n\")\n",
    "        f.write(f\"Daytime RMSE: {day_rmse:.4f}\\n\")\n",
    "        f.write(f\"Daytime MAE: {day_mae:.4f}\\n\")\n",
    "        f.write(f\"Daytime R²: {day_r2:.4f}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TIME-AWARE LSTM MODEL TRAINING AND EVALUATION COMPLETE\")\n",
    "print(f\"All model files and outputs saved to '{model_dir}' directory:\")\n",
    "print(f\"  - Model: {model_dir}/checkpoints/time_aware_lstm_model.pth\")\n",
    "print(f\"  - Data: {model_dir}/data/\")\n",
    "print(f\"  - Plots: {model_dir}/plots/\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
